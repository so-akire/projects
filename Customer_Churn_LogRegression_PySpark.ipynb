{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A marketing agency has many customers that use their service to produce ads for the client/customer websites. They've noticed that they have quite a bit of churn in clients. They basically randomly assign account managers right now, but need to create a machine learning model that will help predict which customers will churn (stop buying their service) so that they can correctly assign the customers most at risk to churn an account manager. \n",
    "\n",
    "Based on historical data, I use logistic regression that will help classify whether or not a customer churned. Then the company can test this against incoming data for future customers to predict which customers will churn and assign them an account manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. builder.appName('churn').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('customer_churn.csv', header = 'True', inferSchema = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+\n",
      "|summary|              Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|              Churn|\n",
      "+-------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+\n",
      "|  count|              900|              900|               900|              900|               900|                900|\n",
      "|   mean|41.81666666666667|10062.82403333334|0.4811111111111111| 5.27315555555555| 8.587777777777777|0.16666666666666666|\n",
      "| stddev|6.127560416916251|2408.644531858096|0.4999208935073339|1.274449013194616|1.7648355920350969| 0.3728852122772358|\n",
      "|    min|             22.0|            100.0|                 0|              1.0|               3.0|                  0|\n",
      "|    max|             65.0|         18026.01|                 1|             9.15|              14.0|                  1|\n",
      "+-------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df. select('Age',\n",
    " 'Total_Purchase',\n",
    " 'Account_Manager',\n",
    " 'Years',\n",
    " 'Num_Sites',\n",
    " 'Churn').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Names='Cameron Williams', Age=42.0, Total_Purchase=11066.8, Account_Manager=0, Years=7.22, Num_Sites=8.0, Onboard_date='2013-08-30 07:00:40', Location='10265 Elizabeth Mission Barkerburgh, AK 89518', Company='Harvey LLC', Churn=1)\n",
      "\n",
      "\n",
      "Row(Names='Kevin Mueller', Age=41.0, Total_Purchase=11916.22, Account_Manager=0, Years=6.5, Num_Sites=11.0, Onboard_date='2013-08-13 00:38:46', Location='6157 Frank Gardens Suite 019 Carloshaven, RI 17756', Company='Wilson PLC', Churn=1)\n",
      "\n",
      "\n",
      "Row(Names='Eric Lozano', Age=38.0, Total_Purchase=12884.75, Account_Manager=0, Years=6.67, Num_Sites=12.0, Onboard_date='2016-06-29 06:20:07', Location='1331 Keith Court Alyssahaven, DE 90114', Company='Miller, Johnson and Wallace', Churn=1)\n",
      "\n",
      "\n",
      "Row(Names='Phillip White', Age=42.0, Total_Purchase=8010.76, Account_Manager=0, Years=6.71, Num_Sites=10.0, Onboard_date='2014-04-22 12:43:12', Location='13120 Daniel Mount Angelabury, WY 30645-4695', Company='Smith Inc', Churn=1)\n",
      "\n",
      "\n",
      "Row(Names='Cynthia Norton', Age=37.0, Total_Purchase=9191.58, Account_Manager=0, Years=5.56, Num_Sites=9.0, Onboard_date='2016-01-19 15:31:15', Location='765 Tricia Row Karenshire, MH 71730', Company='Love-Jones', Churn=1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in df.head(5):\n",
    "    print(row)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Thoughts on the columns:\n",
    " *'Names'           : arbitrary, irrelevant\n",
    " *'Age'             : ideally needs to be categorized to avoid overfit\n",
    " *'Total_Purchase'  : looks good\n",
    " *'Account_Manager' : will be excluded from features and evaluated after\n",
    " *'Years'           : looks good\n",
    " *'Num_Sites'       : looks good\n",
    " *'Onboard_date'    : irrelevant as this is B2B client. \n",
    "                     Onboard time (day or night or specific month and day) might be  important in B2C scenario.\n",
    " *'Location'        : needs to explore  \n",
    " *'Company'         : needs to explore  \n",
    " *'Churn'           : looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean=df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.count()    #nothing is dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore column Location:\n",
    "Location could be important factor needs to be count in as one of variables. So I will extract the state code from the address:\n",
    "1) Split each word in Location column\n",
    "2) Get the 2nd word from behind of the list. Somehow [-2] did not work for me so I use getItem() and size()-2\n",
    "   It is a known problem:\n",
    "   https://stackoverflow.com/questions/40467936/how-do-i-get-the-last-item-from-a-list-using-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------------------------------------------------+\n",
      "|Names           |state                                                      |\n",
      "+----------------+-----------------------------------------------------------+\n",
      "|Cameron Williams|[10265, Elizabeth, Mission, Barkerburgh,, AK, 89518]       |\n",
      "|Kevin Mueller   |[6157, Frank, Gardens, Suite, 019, Carloshaven,, RI, 17756]|\n",
      "|Eric Lozano     |[1331, Keith, Court, Alyssahaven,, DE, 90114]              |\n",
      "|Phillip White   |[13120, Daniel, Mount, Angelabury,, WY, 30645-4695]        |\n",
      "|Cynthia Norton  |[765, Tricia, Row, Karenshire,, MH, 71730]                 |\n",
      "+----------------+-----------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_state=df.withColumn('state', split(df['Location'],' '))\n",
    "df_state.select('Names','state').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+-------------------------------------------------------+\n",
      "|Names           |sc |Location                                               |\n",
      "+----------------+---+-------------------------------------------------------+\n",
      "|Cameron Williams|AK |10265 Elizabeth Mission Barkerburgh, AK 89518          |\n",
      "|Kevin Mueller   |RI |6157 Frank Gardens Suite 019 Carloshaven, RI 17756     |\n",
      "|Eric Lozano     |DE |1331 Keith Court Alyssahaven, DE 90114                 |\n",
      "|Phillip White   |WY |13120 Daniel Mount Angelabury, WY 30645-4695           |\n",
      "|Cynthia Norton  |MH |765 Tricia Row Karenshire, MH 71730                    |\n",
      "|Jessica Williams|PR |6187 Olson Mountains East Vincentborough, PR 74359     |\n",
      "|Eric Butler     |IA |4846 Savannah Road West Justin, IA 87713-3460          |\n",
      "|Zachary Walsh   |FM |25271 Roy Expressway Suite 147 Brownport, FM 59852-6150|\n",
      "|Ashlee Carr     |MA |3725 Caroline Stravenue South Christineview, MA 82059  |\n",
      "|Jennifer Lynch  |WI |363 Sandra Lodge Suite 144 South Ann, WI 51655-7561    |\n",
      "+----------------+---+-------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---+-----+\n",
      "|sc |count|\n",
      "+---+-----+\n",
      "|AZ |14   |\n",
      "|SC |18   |\n",
      "|LA |17   |\n",
      "|MN |17   |\n",
      "|AA |44   |\n",
      "|NJ |13   |\n",
      "|DC |15   |\n",
      "|OR |10   |\n",
      "|VA |16   |\n",
      "|RI |14   |\n",
      "+---+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sc=df_state.withColumn('sc',df_state['state'].getItem(size(df_state['state'])-2))\n",
    "df_sc.select('Names','sc','Location').show(10, False)\n",
    "df_sc.groupBy('sc').count().show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of state code is 62:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sc.groupBy('sc').count().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore column Company:\n",
    "-as there are same companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "873"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sc.groupBy('Company').count().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             Company|count|\n",
      "+--------------------+-----+\n",
      "|Miller, Johnson a...|    1|\n",
      "|Hunter, Reyes and...|    1|\n",
      "|          Obrien PLC|    1|\n",
      "|            Soto PLC|    2|\n",
      "|            Todd LLC|    1|\n",
      "|Smith, Marshall a...|    1|\n",
      "|           Smith PLC|    1|\n",
      "|          Hall Group|    1|\n",
      "|Freeman, Lam and ...|    1|\n",
      "|       Smith-Carroll|    1|\n",
      "|Hall, Hernandez a...|    1|\n",
      "|          Cannon Inc|    1|\n",
      "|        White-Dennis|    1|\n",
      "|Wilson, Collins a...|    1|\n",
      "|Jennings, Gates a...|    1|\n",
      "|     Campbell-Willis|    1|\n",
      "|    Martinez-Roberts|    1|\n",
      "|        Robinson PLC|    1|\n",
      "|          Barton Inc|    1|\n",
      "|Hernandez, Middle...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dc=df_sc.groupBy('Company').count()\n",
    "dc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|       Company|count|\n",
      "+--------------+-----+\n",
      "|      Soto PLC|    2|\n",
      "|     Ortiz Ltd|    2|\n",
      "|     Evans LLC|    2|\n",
      "|   Davis Group|    2|\n",
      "|     Jones LLC|    2|\n",
      "|Davis and Sons|    2|\n",
      "|     Smith Inc|    2|\n",
      "|     Gates Ltd|    2|\n",
      "|     Smith Ltd|    2|\n",
      "|  Williams LLC|    2|\n",
      "|      Rice PLC|    2|\n",
      "|      Webb PLC|    2|\n",
      "|    Wilson PLC|    3|\n",
      "|    Nelson LLC|    2|\n",
      "|   Smith Group|    2|\n",
      "|Smith and Sons|    2|\n",
      "|      King LLC|    2|\n",
      "|Anderson Group|    4|\n",
      "|    Walker Ltd|    2|\n",
      "|Perry and Sons|    2|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc.filter(dc['count']>1).show()\n",
    "dc.filter(dc['count']>1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+-----+---+----+\n",
      "|       Company|          Names|Churn| sc| Age|\n",
      "+--------------+---------------+-----+---+----+\n",
      "|    Wilson PLC|  Kevin Mueller|    1| RI|41.0|\n",
      "|      Soto PLC|     Misty Tate|    0| AS|40.0|\n",
      "|Anderson Group|Darin Alexander|    0| MS|43.0|\n",
      "|Anderson Group|   Richard Bell|    0| SC|47.0|\n",
      "|      Soto PLC|  Jodi Thompson|    0| NM|35.0|\n",
      "|    Wilson PLC|  David Sanders|    0| AP|41.0|\n",
      "|    Wilson PLC|    Linda Moore|    0| FM|40.0|\n",
      "|Anderson Group|     Lisa Munoz|    0| NV|39.0|\n",
      "|Anderson Group|  Monica Graham|    0| WI|40.0|\n",
      "+--------------+---------------+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sc.filter( (df_sc['Company'] == 'Anderson Group')|\n",
    "              (df_sc['Company'] == 'Soto PLC')|\n",
    "              (df_sc['Company'] == 'Wilson PLC')).select('Company','Names','Churn', 'sc', 'Age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding:There are only 23 out of 873 companies which have more than 1 account. Even though coming from the same company but they are from differnt office location and different contact person. So it seems company name is not significant as variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Age column:\n",
    "There is quite range of age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col, lit, collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----+\n",
      "|  Age_range|              Names| Age|\n",
      "+-----------+-------------------+----+\n",
      "|40.0 - 45.0|   Cameron Williams|42.0|\n",
      "|40.0 - 45.0|      Kevin Mueller|41.0|\n",
      "|35.0 - 40.0|        Eric Lozano|38.0|\n",
      "|40.0 - 45.0|      Phillip White|42.0|\n",
      "|35.0 - 40.0|     Cynthia Norton|37.0|\n",
      "|45.0 - 50.0|   Jessica Williams|48.0|\n",
      "|40.0 - 45.0|        Eric Butler|44.0|\n",
      "|30.0 - 35.0|      Zachary Walsh|32.0|\n",
      "|40.0 - 45.0|        Ashlee Carr|43.0|\n",
      "|40.0 - 45.0|     Jennifer Lynch|40.0|\n",
      "|30.0 - 35.0|       Paula Harris|30.0|\n",
      "|45.0 - 50.0|     Bruce Phillips|45.0|\n",
      "|45.0 - 50.0|       Craig Garner|45.0|\n",
      "|40.0 - 45.0|       Nicole Olson|40.0|\n",
      "|40.0 - 45.0|     Harold Griffin|41.0|\n",
      "|35.0 - 40.0|       James Wright|38.0|\n",
      "|45.0 - 50.0|      Doris Wilkins|45.0|\n",
      "|40.0 - 45.0|Katherine Carpenter|43.0|\n",
      "|50.0 - 55.0|     Lindsay Martin|53.0|\n",
      "|45.0 - 50.0|        Kathy Curry|46.0|\n",
      "+-----------+-------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interval = 5\n",
    "df4= df_sc.withColumn(\"Age_range\",df_sc[\"Age\"] - (df_sc[\"Age\"] % interval))\n",
    "df5= df4.withColumn(\"Age_range\", concat(df4[\"Age_range\"], lit(\" - \"), df4[\"Age_range\"]+ interval))\n",
    "df5.select('Age_range','Names','Age').show()\n",
    "#showing names per group\n",
    "#df5.groupBy(\"range\").agg(collect_list(\"Names\")).show(5,False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|  Age_range|count|\n",
      "+-----------+-----+\n",
      "|20.0 - 25.0|    1|\n",
      "|25.0 - 30.0|   18|\n",
      "|30.0 - 35.0|   81|\n",
      "|35.0 - 40.0|  218|\n",
      "|40.0 - 45.0|  288|\n",
      "|45.0 - 50.0|  199|\n",
      "|50.0 - 55.0|   72|\n",
      "|55.0 - 60.0|   21|\n",
      "|60.0 - 65.0|    1|\n",
      "|65.0 - 70.0|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.groupBy(\"Age_range\").count().orderBy('Age_range').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on exploration above, I will exclude Names, Company, and Onboard_date. \n",
    "Location & state have been replaced by sc col. \n",
    "Age has been replace by Age_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn',\n",
       " 'state',\n",
       " 'sc',\n",
       " 'Age_range']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data=df5.select('Names',\n",
    " #'Age',\n",
    " 'Total_Purchase',\n",
    " 'Account_Manager',\n",
    " 'Years',\n",
    " 'Num_Sites',\n",
    " #'Onboard_date',\n",
    " #'Location',\n",
    " 'Company',\n",
    " 'Churn',\n",
    " #'state',\n",
    " 'sc',\n",
    " 'Age_range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      " |-- sc: string (nullable = true)\n",
      " |-- Age_range: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Categorical Column:\n",
    "SC & Age_range need string indexer and need to be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
    "                                OneHotEncoder,StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_indexer = StringIndexer (inputCol= 'sc', outputCol= 'scIndex')\n",
    "sc_encoder = OneHotEncoder (inputCol= 'scIndex', outputCol= 'scVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_indexer = StringIndexer (inputCol= 'Age_range', outputCol= 'ageIndex')\n",
    "age_encoder = OneHotEncoder (inputCol= 'ageIndex', outputCol= 'ageVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols= ['ageVec',\n",
    "                                        'Total_Purchase',\n",
    "                                        #'Account_Manager',\n",
    "                                        'Years',\n",
    "                                        'Num_Sites',\n",
    "                                        #'Churn', \n",
    "                                        'scVec'], \n",
    "                             outputCol='features' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logrec_churn = LogisticRegression(featuresCol='features', labelCol='Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages= [sc_indexer, age_indexer,\n",
    "                            sc_encoder, age_encoder,\n",
    "                            assembler , logrec_churn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the Data, fit the train Data, and transform it with test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainD,testD = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model = pipeline.fit(trainD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = fit_model.transform(testD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      " |-- sc: string (nullable = true)\n",
      " |-- Age_range: string (nullable = true)\n",
      " |-- scIndex: double (nullable = false)\n",
      " |-- ageIndex: double (nullable = false)\n",
      " |-- scVec: vector (nullable = true)\n",
      " |-- ageVec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', \n",
    "                                     labelCol ='Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------------+\n",
      "|Churn|prediction|Account_Manager|\n",
      "+-----+----------+---------------+\n",
      "|    0|       0.0|              0|\n",
      "|    1|       1.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              1|\n",
      "|    1|       0.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       1.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    1|       0.0|              1|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    1|       1.0|              1|\n",
      "|    0|       0.0|              1|\n",
      "|    1|       1.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       1.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       1.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       1.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    1|       0.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    1|       0.0|              0|\n",
      "|    1|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       1.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              0|\n",
      "|    1|       1.0|              0|\n",
      "|    1|       1.0|              1|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    1|       0.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              1|\n",
      "|    1|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    1|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    1|       1.0|              1|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    1|       1.0|              1|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    1|       1.0|              1|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    0|       0.0|              0|\n",
      "|    0|       0.0|              1|\n",
      "|    1|       1.0|              1|\n",
      "|    1|       0.0|              0|\n",
      "|    1|       1.0|              0|\n",
      "+-----+----------+---------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select('Churn','prediction','Account_Manager').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area under ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I often get error with evaluate: \n",
    "     An error occurred while calling o5717.evaluate.\n",
    "    https://stackoverflow.com/questions/54546513/py4jjavaerror-an-error-occurred-while-calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o496.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 169.0 failed 1 times, most recent failure: Lost task 0.0 in stage 169.0 (TID 809, 192.168.0.25, executor driver): org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$3132/168157933: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:192)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Unseen label: 20.0 - 25.0. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1227)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:182)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:163)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:165)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:165)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:245)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:103)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:114)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:122)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$3132/168157933: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:192)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen label: 20.0 - 25.0. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n\t... 19 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-dbf2c8181f4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAUC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \"\"\"\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o496.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 169.0 failed 1 times, most recent failure: Lost task 0.0 in stage 169.0 (TID 809, 192.168.0.25, executor driver): org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$3132/168157933: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:192)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Unseen label: 20.0 - 25.0. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1227)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:182)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:163)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:165)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:165)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:245)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:103)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:114)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:122)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$3132/168157933: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:192)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen label: 20.0 - 25.0. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n\t... 19 more\n"
     ]
    }
   ],
   "source": [
    "AUC = my_eval.evaluate(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7025786713286714"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " including account_manager col in the future and final_data is 0.65\n",
    " exclude Manager in the future and final_data :0.68\n",
    " exclude Manager in the feature :0.70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data= spark.read.csv('new_customers.csv', header=True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------+\n",
      "|        Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location| Company|\n",
      "+-------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------+\n",
      "|Andrew Mccall|37.0|       9935.53|              1| 7.71|      8.0|2011-08-29 18:37:54|38612 Johnny Stra...|King Ltd|\n",
      "+-------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+----------------+\n",
      "|summary|        Names|               Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|       Onboard_date|            Location|         Company|\n",
      "+-------+-------------+------------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+----------------+\n",
      "|  count|            6|                 6|                6|                 6|                6|                 6|                  6|                   6|               6|\n",
      "|   mean|         null|35.166666666666664|7607.156666666667|0.8333333333333334|6.808333333333334|12.333333333333334|               null|                null|            null|\n",
      "| stddev|         null| 15.71517313511584|4346.008232825459| 0.408248290463863|3.708737880555414|3.3862466931200785|               null|                null|            null|\n",
      "|    min|Andrew Mccall|              22.0|            100.0|                 0|              1.0|               8.0|2006-12-11 07:48:13|085 Austin Views ...|Barron-Robertson|\n",
      "|    max| Taylor Young|              65.0|         13147.71|                 1|             10.0|              15.0|2016-10-28 05:32:13|Unit 0789 Box 073...|        Wood LLC|\n",
      "+-------+-------------+------------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd_state=new_data.withColumn('state', split(new_data['Location'],' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd_sc=nd_state.withColumn('sc',nd_state['state'].getItem(size(nd_state['state'])-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----+\n",
      "|  Age_range|         Names| Age|\n",
      "+-----------+--------------+----+\n",
      "|35.0 - 40.0| Andrew Mccall|37.0|\n",
      "|20.0 - 25.0|Michele Wright|23.0|\n",
      "|65.0 - 70.0|  Jeremy Chang|65.0|\n",
      "|30.0 - 35.0|Megan Ferguson|32.0|\n",
      "|30.0 - 35.0|  Taylor Young|32.0|\n",
      "|20.0 - 25.0| Jessica Drake|22.0|\n",
      "+-----------+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interval = 5\n",
    "nd1= nd_sc.withColumn(\"Age_range\",nd_sc[\"Age\"] - (nd_sc[\"Age\"] % interval))\n",
    "nd2= nd1.withColumn(\"Age_range\", concat(nd1[\"Age_range\"], lit(\" - \"), nd1[\"Age_range\"]+ interval))\n",
    "nd2.select('Age_range','Names','Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'state',\n",
       " 'sc',\n",
       " 'Age_range']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDataSet = nd2.select('Names',\n",
    " #'Age',\n",
    " 'Total_Purchase',\n",
    " 'Account_Manager',\n",
    " 'Years',\n",
    " 'Num_Sites',\n",
    " #'Onboard_date',\n",
    " #'Location',\n",
    " 'Company',                  \n",
    " #'state',\n",
    " 'sc',\n",
    " 'Age_range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result = fit_model.transform(newDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- sc: string (nullable = true)\n",
      " |-- Age_range: string (nullable = true)\n",
      " |-- scIndex: double (nullable = false)\n",
      " |-- ageIndex: double (nullable = false)\n",
      " |-- scVec: vector (nullable = true)\n",
      " |-- ageVec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I often get error with showing result: \n",
    "    An error occurred while calling o5800.showString."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Names='Andrew Mccall', Total_Purchase=9935.53, Account_Manager=1, Years=7.71, Num_Sites=8.0, Company='King Ltd', sc='WI', Age_range='35.0 - 40.0', scIndex=38.0, ageIndex=1.0, scVec=SparseVector(61, {38: 1.0}), ageVec=SparseVector(9, {1: 1.0}), features=SparseVector(73, {1: 1.0, 9: 9935.53, 10: 7.71, 11: 8.0, 50: 1.0}), rawPrediction=DenseVector([3.0222, -3.0222]), probability=DenseVector([0.9536, 0.0464]), prediction=0.0)\n",
      "\n",
      "\n",
      "Row(Names='Michele Wright', Total_Purchase=7526.94, Account_Manager=1, Years=9.28, Num_Sites=15.0, Company='Cannon-Benson', sc='ME', Age_range='20.0 - 25.0', scIndex=18.0, ageIndex=7.0, scVec=SparseVector(61, {18: 1.0}), ageVec=SparseVector(9, {7: 1.0}), features=SparseVector(73, {7: 1.0, 9: 7526.94, 10: 9.28, 11: 15.0, 30: 1.0}), rawPrediction=DenseVector([15.2072, -15.2072]), probability=DenseVector([1.0, 0.0]), prediction=0.0)\n",
      "\n",
      "\n",
      "Row(Names='Jeremy Chang', Total_Purchase=100.0, Account_Manager=1, Years=1.0, Num_Sites=15.0, Company='Barron-Robertson', sc='WY', Age_range='65.0 - 70.0', scIndex=59.0, ageIndex=9.0, scVec=SparseVector(61, {59: 1.0}), ageVec=SparseVector(9, {}), features=SparseVector(73, {9: 100.0, 10: 1.0, 11: 15.0, 71: 1.0}), rawPrediction=DenseVector([3.2506, -3.2506]), probability=DenseVector([0.9627, 0.0373]), prediction=0.0)\n",
      "\n",
      "\n",
      "Row(Names='Megan Ferguson', Total_Purchase=6487.5, Account_Manager=0, Years=9.4, Num_Sites=14.0, Company='Sexton-Golden', sc='NC', Age_range='30.0 - 35.0', scIndex=51.0, ageIndex=3.0, scVec=SparseVector(61, {51: 1.0}), ageVec=SparseVector(9, {3: 1.0}), features=SparseVector(73, {3: 1.0, 9: 6487.5, 10: 9.4, 11: 14.0, 63: 1.0}), rawPrediction=DenseVector([-5.7889, 5.7889]), probability=DenseVector([0.0031, 0.9969]), prediction=1.0)\n",
      "\n",
      "\n",
      "Row(Names='Taylor Young', Total_Purchase=13147.71, Account_Manager=1, Years=10.0, Num_Sites=8.0, Company='Wood LLC', sc='AP', Age_range='30.0 - 35.0', scIndex=1.0, ageIndex=3.0, scVec=SparseVector(61, {1: 1.0}), ageVec=SparseVector(9, {3: 1.0}), features=SparseVector(73, {3: 1.0, 9: 13147.71, 10: 10.0, 11: 8.0, 13: 1.0}), rawPrediction=DenseVector([0.1024, -0.1024]), probability=DenseVector([0.5256, 0.4744]), prediction=0.0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in new_result.head(5):\n",
    "    print (row)\n",
    "    print ('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|         Names|         Company|\n",
      "+--------------+----------------+\n",
      "| Andrew Mccall|        King Ltd|\n",
      "|Michele Wright|   Cannon-Benson|\n",
      "|  Jeremy Chang|Barron-Robertson|\n",
      "|Megan Ferguson|   Sexton-Golden|\n",
      "|  Taylor Young|        Wood LLC|\n",
      "| Jessica Drake|   Parks-Robbins|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.select('Names','Company').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Received Py4J error when trying to show dataframe. However we could see from head() that the predict\n",
    "         King Ltd|0\n",
    "|   Cannon-Benson|0\n",
    "|Barron-Robertson|0\n",
    "|   Sexton-Golden|1\n",
    "|        Wood LLC|0\n",
    "|   Parks-Robbins|?\n",
    "\n",
    "Sexton Golden will churn.We don't have info of Parks-Robbins since head only displayed first 5. It gives error when  I put head(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_result.select('Names','prediction').show() #Received Py4J error when trying to show dataframe.  A known problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
